{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset pubmed_qa (C:/Users/dungb/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/dd4c39f031a958c7e782595fa4dd1b1330484e8bbadd4d9212e5046f27e68924)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['pubid', 'question', 'context', 'long_answer', 'final_decision'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset  # !pip install datasets\n",
    "pubmed = load_dataset(\n",
    "   'pubmed_qa',\n",
    "   'pqa_labeled',\n",
    "   split='train'\n",
    ")\n",
    "pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programmed cell death (PCD) is the regulated death of cells within an organism. The lace plant (Aponogeton madagascariensis) produces perforations in its leaves through PCD. The leaves of the plant consist of a latticework of longitudinal and transverse veins enclosing areoles. PCD occurs in the cel...\n",
      "Assessment of visual acuity depends on the optotypes used for measurement. The ability to recognize different optotypes differs even if their critical details appear under the same visual angle. Since optotypes are evaluated on individuals with good visual acuity and without eye disorders, differenc...\n"
     ]
    }
   ],
   "source": [
    "contexts = []\n",
    "# loop through the context passages\n",
    "for record in pubmed['context']:\n",
    "   # join context passages for each question and append to contexts list\n",
    "   contexts.append('\\n'.join(record['contexts']))\n",
    "# view some of the contexts\n",
    "for context in contexts[:2]:\n",
    "   print(f\"{context[:300]}...\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse vector\n",
    "##### Lưu ý là sparse vector chưa được tối ưu, nên dùng với thuật toán như là BM25 hoặc Splade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast  # !pip install transformers\n",
    "\n",
    "# load bert tokenizer from huggingface\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\n",
    "   'bert-base-uncased'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the context passage\n",
    "inputs = tokenizer(\n",
    "   contexts[0], padding=True, truncation=True,\n",
    "   max_length=512\n",
    ")\n",
    "inputs.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 16984,\n",
       " 3526,\n",
       " 2331,\n",
       " 1006,\n",
       " 7473,\n",
       " 2094,\n",
       " 1007,\n",
       " 2003,\n",
       " 1996,\n",
       " 12222,\n",
       " 2331,\n",
       " 1997,\n",
       " 4442,\n",
       " 2306,\n",
       " 2019,\n",
       " 15923,\n",
       " 1012,\n",
       " 1996,\n",
       " 12922,\n",
       " 3269,\n",
       " 1006,\n",
       " 9706,\n",
       " 17175,\n",
       " 18150,\n",
       " 2239,\n",
       " 11934,\n",
       " 27806,\n",
       " 1007,\n",
       " 7137,\n",
       " 2566,\n",
       " 29278,\n",
       " 10708,\n",
       " 1999,\n",
       " 2049,\n",
       " 3727,\n",
       " 2083,\n",
       " 7473,\n",
       " 2094,\n",
       " 1012,\n",
       " 1996,\n",
       " 3727,\n",
       " 1997,\n",
       " 1996,\n",
       " 3269,\n",
       " 8676,\n",
       " 1997,\n",
       " 1037,\n",
       " 17779,\n",
       " 6198,\n",
       " 1997,\n",
       " 20134,\n",
       " 1998,\n",
       " 18323,\n",
       " 9607,\n",
       " 4372,\n",
       " 20464,\n",
       " 18606,\n",
       " 2024,\n",
       " 29111,\n",
       " 1012,\n",
       " 7473,\n",
       " 2094,\n",
       " 5158,\n",
       " 1999,\n",
       " 1996,\n",
       " 4442,\n",
       " 2012,\n",
       " 1996,\n",
       " 2415,\n",
       " 1997,\n",
       " 2122,\n",
       " 2024,\n",
       " 29111,\n",
       " 1998,\n",
       " 22901,\n",
       " 15436,\n",
       " 2015,\n",
       " 1010,\n",
       " 7458,\n",
       " 3155,\n",
       " 2274,\n",
       " 4442,\n",
       " 2013,\n",
       " 1996,\n",
       " 12436,\n",
       " 28817,\n",
       " 20051,\n",
       " 5397,\n",
       " 1012,\n",
       " 1996,\n",
       " 2535,\n",
       " 1997,\n",
       " 10210,\n",
       " 11663,\n",
       " 15422,\n",
       " 4360,\n",
       " 2076,\n",
       " 7473,\n",
       " 2094,\n",
       " 2038,\n",
       " 2042,\n",
       " 3858,\n",
       " 1999,\n",
       " 4176,\n",
       " 1025,\n",
       " 2174,\n",
       " 1010,\n",
       " 2009,\n",
       " 2038,\n",
       " 2042,\n",
       " 2625,\n",
       " 3273,\n",
       " 2076,\n",
       " 7473,\n",
       " 2094,\n",
       " 1999,\n",
       " 4264,\n",
       " 1012,\n",
       " 1996,\n",
       " 2206,\n",
       " 3259,\n",
       " 3449,\n",
       " 14194,\n",
       " 8524,\n",
       " 4570,\n",
       " 1996,\n",
       " 2535,\n",
       " 1997,\n",
       " 23079,\n",
       " 10949,\n",
       " 2076,\n",
       " 13908,\n",
       " 2135,\n",
       " 12222,\n",
       " 7473,\n",
       " 2094,\n",
       " 1999,\n",
       " 24269,\n",
       " 1999,\n",
       " 1037,\n",
       " 1012,\n",
       " 11934,\n",
       " 27806,\n",
       " 1012,\n",
       " 1037,\n",
       " 2309,\n",
       " 2024,\n",
       " 9890,\n",
       " 2306,\n",
       " 1037,\n",
       " 3332,\n",
       " 2754,\n",
       " 7053,\n",
       " 1006,\n",
       " 7473,\n",
       " 2094,\n",
       " 2003,\n",
       " 10066,\n",
       " 1007,\n",
       " 2001,\n",
       " 4055,\n",
       " 2046,\n",
       " 2093,\n",
       " 2752,\n",
       " 2241,\n",
       " 2006,\n",
       " 1996,\n",
       " 14967,\n",
       " 1997,\n",
       " 7473,\n",
       " 2094,\n",
       " 1025,\n",
       " 4442,\n",
       " 2008,\n",
       " 2097,\n",
       " 2025,\n",
       " 13595,\n",
       " 7473,\n",
       " 2094,\n",
       " 1006,\n",
       " 27937,\n",
       " 19797,\n",
       " 1007,\n",
       " 1010,\n",
       " 4442,\n",
       " 1999,\n",
       " 2220,\n",
       " 5711,\n",
       " 1997,\n",
       " 7473,\n",
       " 2094,\n",
       " 1006,\n",
       " 4958,\n",
       " 19797,\n",
       " 1007,\n",
       " 1010,\n",
       " 1998,\n",
       " 4442,\n",
       " 1999,\n",
       " 2397,\n",
       " 5711,\n",
       " 1997,\n",
       " 7473,\n",
       " 2094,\n",
       " 1006,\n",
       " 6948,\n",
       " 19797,\n",
       " 1007,\n",
       " 1012,\n",
       " 3332,\n",
       " 2754,\n",
       " 3727,\n",
       " 2020,\n",
       " 9702,\n",
       " 2007,\n",
       " 1996,\n",
       " 23079,\n",
       " 18554,\n",
       " 10210,\n",
       " 4140,\n",
       " 22648,\n",
       " 5484,\n",
       " 2417,\n",
       " 4642,\n",
       " 2595,\n",
       " 7352,\n",
       " 1998,\n",
       " 8920,\n",
       " 1012,\n",
       " 23079,\n",
       " 10949,\n",
       " 2020,\n",
       " 3972,\n",
       " 3170,\n",
       " 4383,\n",
       " 2046,\n",
       " 2176,\n",
       " 7236,\n",
       " 1006,\n",
       " 23290,\n",
       " 1011,\n",
       " 1049,\n",
       " 2549,\n",
       " 1007,\n",
       " 2241,\n",
       " 2006,\n",
       " 6459,\n",
       " 2164,\n",
       " 4353,\n",
       " 1010,\n",
       " 9587,\n",
       " 18724,\n",
       " 1010,\n",
       " 1998,\n",
       " 10804,\n",
       " 4022,\n",
       " 1006,\n",
       " 1158,\n",
       " 29738,\n",
       " 2213,\n",
       " 1007,\n",
       " 1012,\n",
       " 1037,\n",
       " 8694,\n",
       " 2140,\n",
       " 4632,\n",
       " 4710,\n",
       " 3662,\n",
       " 26872,\n",
       " 1050,\n",
       " 28911,\n",
       " 1999,\n",
       " 1037,\n",
       " 17978,\n",
       " 2058,\n",
       " 2122,\n",
       " 23079,\n",
       " 5711,\n",
       " 1012,\n",
       " 10381,\n",
       " 10626,\n",
       " 7361,\n",
       " 8523,\n",
       " 3215,\n",
       " 1998,\n",
       " 9099,\n",
       " 24887,\n",
       " 19098,\n",
       " 8017,\n",
       " 14119,\n",
       " 2020,\n",
       " 2036,\n",
       " 8920,\n",
       " 2478,\n",
       " 2444,\n",
       " 3526,\n",
       " 12126,\n",
       " 1012,\n",
       " 1996,\n",
       " 2825,\n",
       " 5197,\n",
       " 1997,\n",
       " 23079,\n",
       " 2566,\n",
       " 4168,\n",
       " 8010,\n",
       " 6653,\n",
       " 18499,\n",
       " 2063,\n",
       " 1006,\n",
       " 13866,\n",
       " 2361,\n",
       " 1007,\n",
       " 4195,\n",
       " 2076,\n",
       " 7473,\n",
       " 2094,\n",
       " 2001,\n",
       " 17351,\n",
       " 8920,\n",
       " 3081,\n",
       " 1999,\n",
       " 24269,\n",
       " 22330,\n",
       " 20464,\n",
       " 2891,\n",
       " 17822,\n",
       " 3170,\n",
       " 1037,\n",
       " 1006,\n",
       " 27804,\n",
       " 1007,\n",
       " 3949,\n",
       " 1012,\n",
       " 2023,\n",
       " 3949,\n",
       " 4504,\n",
       " 1999,\n",
       " 12922,\n",
       " 3269,\n",
       " 3727,\n",
       " 2007,\n",
       " 1037,\n",
       " 6022,\n",
       " 2896,\n",
       " 2193,\n",
       " 1997,\n",
       " 2566,\n",
       " 29278,\n",
       " 10708,\n",
       " 4102,\n",
       " 2000,\n",
       " 7711,\n",
       " 1010,\n",
       " 1998,\n",
       " 2008,\n",
       " 6913,\n",
       " 23079,\n",
       " 10949,\n",
       " 2714,\n",
       " 2000,\n",
       " 2008,\n",
       " 1997,\n",
       " 2512,\n",
       " 1011,\n",
       " 7473,\n",
       " 2094,\n",
       " 4442,\n",
       " 1012,\n",
       " 102]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = inputs['input_ids']\n",
    "input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{101: 1,\n",
       " 16984: 1,\n",
       " 3526: 2,\n",
       " 2331: 2,\n",
       " 1006: 10,\n",
       " 7473: 13,\n",
       " 2094: 13,\n",
       " 1007: 10,\n",
       " 2003: 2,\n",
       " 1996: 13,\n",
       " 12222: 2,\n",
       " 1997: 13,\n",
       " 4442: 7,\n",
       " 2306: 2,\n",
       " 2019: 1,\n",
       " 15923: 1,\n",
       " 1012: 14,\n",
       " 12922: 2,\n",
       " 3269: 3,\n",
       " 9706: 1,\n",
       " 17175: 1,\n",
       " 18150: 1,\n",
       " 2239: 1,\n",
       " 11934: 2,\n",
       " 27806: 2,\n",
       " 7137: 1,\n",
       " 2566: 3,\n",
       " 29278: 2,\n",
       " 10708: 2,\n",
       " 1999: 11,\n",
       " 2049: 1,\n",
       " 3727: 4,\n",
       " 2083: 1,\n",
       " 8676: 1,\n",
       " 1037: 8,\n",
       " 17779: 1,\n",
       " 6198: 1,\n",
       " 20134: 1,\n",
       " 1998: 7,\n",
       " 18323: 1,\n",
       " 9607: 1,\n",
       " 4372: 1,\n",
       " 20464: 2,\n",
       " 18606: 1,\n",
       " 2024: 3,\n",
       " 29111: 2,\n",
       " 5158: 1,\n",
       " 2012: 1,\n",
       " 2415: 1,\n",
       " 2122: 2,\n",
       " 22901: 1,\n",
       " 15436: 1,\n",
       " 2015: 1,\n",
       " 1010: 7,\n",
       " 7458: 1,\n",
       " 3155: 1,\n",
       " 2274: 1,\n",
       " 2013: 1,\n",
       " 12436: 1,\n",
       " 28817: 1,\n",
       " 20051: 1,\n",
       " 5397: 1,\n",
       " 2535: 2,\n",
       " 10210: 2,\n",
       " 11663: 1,\n",
       " 15422: 1,\n",
       " 4360: 1,\n",
       " 2076: 4,\n",
       " 2038: 2,\n",
       " 2042: 2,\n",
       " 3858: 1,\n",
       " 4176: 1,\n",
       " 1025: 2,\n",
       " 2174: 1,\n",
       " 2009: 1,\n",
       " 2625: 1,\n",
       " 3273: 1,\n",
       " 4264: 1,\n",
       " 2206: 1,\n",
       " 3259: 1,\n",
       " 3449: 1,\n",
       " 14194: 1,\n",
       " 8524: 1,\n",
       " 4570: 1,\n",
       " 23079: 6,\n",
       " 10949: 3,\n",
       " 13908: 1,\n",
       " 2135: 1,\n",
       " 24269: 2,\n",
       " 2309: 1,\n",
       " 9890: 1,\n",
       " 3332: 2,\n",
       " 2754: 2,\n",
       " 7053: 1,\n",
       " 10066: 1,\n",
       " 2001: 2,\n",
       " 4055: 1,\n",
       " 2046: 2,\n",
       " 2093: 1,\n",
       " 2752: 1,\n",
       " 2241: 2,\n",
       " 2006: 2,\n",
       " 14967: 1,\n",
       " 2008: 3,\n",
       " 2097: 1,\n",
       " 2025: 1,\n",
       " 13595: 1,\n",
       " 27937: 1,\n",
       " 19797: 3,\n",
       " 2220: 1,\n",
       " 5711: 3,\n",
       " 4958: 1,\n",
       " 2397: 1,\n",
       " 6948: 1,\n",
       " 2020: 3,\n",
       " 9702: 1,\n",
       " 2007: 2,\n",
       " 18554: 1,\n",
       " 4140: 1,\n",
       " 22648: 1,\n",
       " 5484: 1,\n",
       " 2417: 1,\n",
       " 4642: 1,\n",
       " 2595: 1,\n",
       " 7352: 1,\n",
       " 8920: 3,\n",
       " 3972: 1,\n",
       " 3170: 2,\n",
       " 4383: 1,\n",
       " 2176: 1,\n",
       " 7236: 1,\n",
       " 23290: 1,\n",
       " 1011: 2,\n",
       " 1049: 1,\n",
       " 2549: 1,\n",
       " 6459: 1,\n",
       " 2164: 1,\n",
       " 4353: 1,\n",
       " 9587: 1,\n",
       " 18724: 1,\n",
       " 10804: 1,\n",
       " 4022: 1,\n",
       " 1158: 1,\n",
       " 29738: 1,\n",
       " 2213: 1,\n",
       " 8694: 1,\n",
       " 2140: 1,\n",
       " 4632: 1,\n",
       " 4710: 1,\n",
       " 3662: 1,\n",
       " 26872: 1,\n",
       " 1050: 1,\n",
       " 28911: 1,\n",
       " 17978: 1,\n",
       " 2058: 1,\n",
       " 10381: 1,\n",
       " 10626: 1,\n",
       " 7361: 1,\n",
       " 8523: 1,\n",
       " 3215: 1,\n",
       " 9099: 1,\n",
       " 24887: 1,\n",
       " 19098: 1,\n",
       " 8017: 1,\n",
       " 14119: 1,\n",
       " 2036: 1,\n",
       " 2478: 1,\n",
       " 2444: 1,\n",
       " 12126: 1,\n",
       " 2825: 1,\n",
       " 5197: 1,\n",
       " 4168: 1,\n",
       " 8010: 1,\n",
       " 6653: 1,\n",
       " 18499: 1,\n",
       " 2063: 1,\n",
       " 13866: 1,\n",
       " 2361: 1,\n",
       " 4195: 1,\n",
       " 17351: 1,\n",
       " 3081: 1,\n",
       " 22330: 1,\n",
       " 2891: 1,\n",
       " 17822: 1,\n",
       " 27804: 1,\n",
       " 3949: 2,\n",
       " 2023: 1,\n",
       " 4504: 1,\n",
       " 6022: 1,\n",
       " 2896: 1,\n",
       " 2193: 1,\n",
       " 4102: 1,\n",
       " 2000: 2,\n",
       " 7711: 1,\n",
       " 6913: 1,\n",
       " 2714: 1,\n",
       " 2512: 1,\n",
       " 102: 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# convert the input_ids list to a dictionary of key to frequency values\n",
    "sparse_vec = dict(Counter(input_ids))\n",
    "sparse_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(input_batch):\n",
    " # store a batch of sparse embeddings\n",
    "   sparse_emb = []\n",
    "   # iterate through input batch\n",
    "   for token_ids in input_batch:\n",
    "       indices = []\n",
    "       values = []\n",
    "       # convert the input_ids list to a dictionary of key to frequency values\n",
    "       d = dict(Counter(token_ids))\n",
    "       for idx in d:\n",
    "            indices.append(idx)\n",
    "            values.append(d[idx])\n",
    "       sparse_emb.append({'indices': indices, 'values': values})\n",
    "   # return sparse_emb list\n",
    "   return sparse_emb\n",
    "\n",
    "\n",
    "def generate_sparse_vectors(context_batch):\n",
    "    # create batch of input_ids\n",
    "    inputs = tokenizer(\n",
    "            context_batch, padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512, special_tokens=False\n",
    "    )['input_ids']\n",
    "    # create sparse dictionaries\n",
    "    sparse_embeds = build_dict(inputs)\n",
    "    return sparse_embeds\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# load a sentence transformer model from huggingface\n",
    "model = SentenceTransformer(\n",
    "   'multi-qa-MiniLM-L6-cos-v1'\n",
    ")\n",
    "\n",
    "emb = model.encode(contexts[0])\n",
    "emb.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.index.Index at 0x125710448b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pinecone  # !pip install pinecone-client\n",
    "import os\n",
    "PINECONE_API_KEY = os.environ['PINECONE_API_KEY']\n",
    "PINECONE_API_ENV = os.environ['PINECONE_API_ENV']\n",
    "pinecone.init(\n",
    "   api_key=PINECONE_API_KEY,  # app.pinecone.io\n",
    "   environment=PINECONE_API_ENV # find next to api key in console\n",
    ")\n",
    "# choose a name for your index\n",
    "index_name = \"index1\"\n",
    "pinecone.Index(index_name)\n",
    "# create the index\n",
    "# pinecone.create_index(\n",
    "#    index_name = index_name,\n",
    "#    dimension = 384,  # dimensionality of dense model\n",
    "#    metric = \"dotproduct\",\n",
    "#    pod_type = \"s1\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5e0244501e471f8fa65abace777f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\dungb\\AppData\\Local\\Temp\\ipykernel_30948\\2015152028.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">17</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\dungb\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_30948\\\\2015152028.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\dungb\\AppData\\Local\\Temp\\ipykernel_30948\\2070478225.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">20</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate_sparse_vectors</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'C:\\\\Users\\\\dungb\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_30948\\\\2070478225.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">d:\\Work-Programe\\Python\\lib\\site-packages\\transformers\\tokenization_utils_base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2530</span> in        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2527 │   │   │   # input mode in this case.</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2528 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._in_target_context_manager:                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2529 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._switch_to_input_mode()                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2530 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>encodings = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call_one(text=text, text_pair=text_pair, **all_kwargs)      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2531 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> text_target <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2532 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._switch_to_target_mode()                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2533 │   │   │   </span>target_encodings = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call_one(text=text_target, text_pair=text_pair_targ  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">d:\\Work-Programe\\Python\\lib\\site-packages\\transformers\\tokenization_utils_base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2616</span> in        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_one</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2613 │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\" {</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(text_pair)<span style=\"color: #808000; text-decoration-color: #808000\">}.\"</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2614 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2615 │   │   │   </span>batch_text_or_text_pairs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">zip</span>(text, text_pair)) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> text_pair <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">No</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2616 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.batch_encode_plus(                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2617 │   │   │   │   </span>batch_text_or_text_pairs=batch_text_or_text_pairs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2618 │   │   │   │   </span>add_special_tokens=add_special_tokens,                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2619 │   │   │   │   </span>padding=padding,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">d:\\Work-Programe\\Python\\lib\\site-packages\\transformers\\tokenization_utils_base.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2807</span> in        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">batch_encode_plus</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2804 │   │   │   </span>**kwargs,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2805 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2806 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2807 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._batch_encode_plus(                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2808 │   │   │   </span>batch_text_or_text_pairs=batch_text_or_text_pairs,                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2809 │   │   │   </span>add_special_tokens=add_special_tokens,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2810 │   │   │   </span>padding_strategy=padding_strategy,                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PreTrainedTokenizerFast._batch_encode_plus</span><span style=\"font-weight: bold\">()</span> got an unexpected keyword argument <span style=\"color: #008000; text-decoration-color: #008000\">'special_tokens'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\dungb\\AppData\\Local\\Temp\\ipykernel_30948\\2015152028.py\u001b[0m:\u001b[94m17\u001b[0m in \u001b[92m<module>\u001b[0m                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\dungb\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_30948\\\\2015152028.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\dungb\\AppData\\Local\\Temp\\ipykernel_30948\\2070478225.py\u001b[0m:\u001b[94m20\u001b[0m in \u001b[92mgenerate_sparse_vectors\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'C:\\\\Users\\\\dungb\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_30948\\\\2070478225.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33md:\\Work-Programe\\Python\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m:\u001b[94m2530\u001b[0m in        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m__call__\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2527 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# input mode in this case.\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2528 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96mself\u001b[0m._in_target_context_manager:                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2529 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._switch_to_input_mode()                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2530 \u001b[2m│   │   │   \u001b[0mencodings = \u001b[96mself\u001b[0m._call_one(text=text, text_pair=text_pair, **all_kwargs)      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2531 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m text_target \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2532 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._switch_to_target_mode()                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2533 \u001b[0m\u001b[2m│   │   │   \u001b[0mtarget_encodings = \u001b[96mself\u001b[0m._call_one(text=text_target, text_pair=text_pair_targ  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33md:\\Work-Programe\\Python\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m:\u001b[94m2616\u001b[0m in        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call_one\u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2613 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m \u001b[0m\u001b[33m{\u001b[0m\u001b[96mlen\u001b[0m(text_pair)\u001b[33m}\u001b[0m\u001b[33m.\u001b[0m\u001b[33m\"\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2614 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2615 \u001b[0m\u001b[2m│   │   │   \u001b[0mbatch_text_or_text_pairs = \u001b[96mlist\u001b[0m(\u001b[96mzip\u001b[0m(text, text_pair)) \u001b[94mif\u001b[0m text_pair \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2616 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.batch_encode_plus(                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2617 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mbatch_text_or_text_pairs=batch_text_or_text_pairs,                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2618 \u001b[0m\u001b[2m│   │   │   │   \u001b[0madd_special_tokens=add_special_tokens,                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2619 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mpadding=padding,                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33md:\\Work-Programe\\Python\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m:\u001b[94m2807\u001b[0m in        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mbatch_encode_plus\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2804 \u001b[0m\u001b[2m│   │   │   \u001b[0m**kwargs,                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2805 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2806 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2807 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._batch_encode_plus(                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2808 \u001b[0m\u001b[2m│   │   │   \u001b[0mbatch_text_or_text_pairs=batch_text_or_text_pairs,                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2809 \u001b[0m\u001b[2m│   │   │   \u001b[0madd_special_tokens=add_special_tokens,                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2810 \u001b[0m\u001b[2m│   │   │   \u001b[0mpadding_strategy=padding_strategy,                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35mPreTrainedTokenizerFast._batch_encode_plus\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m got an unexpected keyword argument \u001b[32m'special_tokens'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for i in tqdm(range(0, len(contexts), batch_size)):\n",
    "    # find end of batch\n",
    "    i_end = min(i+batch_size, len(contexts))\n",
    "    # extract batch\n",
    "    context_batch = contexts[i:i_end]\n",
    "    # create unique IDs\n",
    "    ids = [str(x) for x in range(i, i_end)]\n",
    "    # add context passages as metadata\n",
    "    meta = [{'context': context} for context in context_batch]\n",
    "    # create dense vectors\n",
    "    dense_embeds = model.encode(context_batch).tolist()\n",
    "    # create sparse vectors\n",
    "    sparse_embeds = generate_sparse_vectors(context_batch)\n",
    "\n",
    "    vectors = []\n",
    "    # loop through the data and create dictionaries for upserts\n",
    "    for _id, sparse, dense, metadata in zip(\n",
    "        ids, sparse_embeds, dense_embeds, meta\n",
    "    ):\n",
    "        vectors.append({\n",
    "            'id': _id,\n",
    "            'sparse_values': sparse,\n",
    "            'values': dense,\n",
    "            'metadata': metadata\n",
    "        })\n",
    "\n",
    "    # upload the documents to the new hybrid index\n",
    "    pinecone.upsert(vectors)\n",
    "\n",
    "# show index description after uploading the documents\n",
    "pinecone.describe_index_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
